{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.losses import *\n",
    "from cbrain.utils import limit_mem\n",
    "from cbrain.layers import *\n",
    "import tensorflow as tf\n",
    "#import tensorflow.math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "#from cbrain.model_diagnostics.model_diagnostics import ModelDiagnostics\n",
    "\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/home/dwalling/miniconda3/envs/cbrain_tf2/pkgs/cuda-toolkit/extras/CUPTI/lib64:$LD_LIBRARY_PATH'\n",
    "\n",
    "import cProfile\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Otherwise tensorflow will use ALL your GPU RAM for no reason\n",
    "#limit_mem()\n",
    "TRAINDIR = '/oasis/projects/nsf/sio134/dwalling/TomData/'\n",
    "DATADIR = '/oasis/projects/nsf/sio134/dwalling/TomData/'\n",
    "PREFIX = '8col009_01_'\n",
    "\n",
    "#import os\n",
    "#os.chdir('/work/00157/walling/projects/cloud_emulator/TomData')\n",
    "\n",
    "scale_dict = load_pickle(DATADIR + '009_Wm2_scaling.pkl')\n",
    "in_vars = load_pickle(DATADIR + '009_Wm2_in_vars.pkl')\n",
    "out_vars = load_pickle(DATADIR + '009_Wm2_out_vars.pkl')\n",
    "dP = load_pickle(DATADIR + '009_Wm2_dP.pkl')\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    data_fn = TRAINDIR+PREFIX+'train.nc',\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+PREFIX+'norm.nc',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_gen = DataGenerator(\n",
    "    data_fn = TRAINDIR+PREFIX+'valid.nc',\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+PREFIX+'norm.nc',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Tensorboard Profiling\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "log_dir=\"/home/dwalling/tbeucler-CBRAIN-CAM/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=\"logs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  0  and NN is  {}\n",
      "WARNING:tensorflow:From /home/dwalling/miniconda3/envs/cbrain_tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 304)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               156160    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 218)               111834    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 218)               0         \n",
      "=================================================================\n",
      "Total params: 1,318,618\n",
      "Trainable params: 1,318,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "NN is  None\n",
      "Epoch 1/2\n",
      "15376/41376 [==========>...................] - ETA: 1:01:53 - loss: 290.2701 - mean_squared_error: 290.2709"
     ]
    }
   ],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "alpha_array = [0] #,0.01,0.25,0.5,0.75,0.99,1] # Loop over weight given to MSE and conservation constraints\n",
    "Nep = 2 #20\n",
    "\n",
    "for alpha in alpha_array:\n",
    "    NN = {}\n",
    "    print('alpha = ',str(alpha),' and NN is ',NN)\n",
    "    graph = tf.Graph()\n",
    "    with tf.compat.v1.Session(graph=graph):\n",
    "        \n",
    "        # 1) Create model\n",
    "        # Unconstrained model with 5 dense layers (Notebook 009)\n",
    "        inpU = Input(shape=(304,))\n",
    "        densout = Dense(512, activation='linear')(inpU)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "        for i in range (4):\n",
    "            densout = Dense(512, activation='linear')(densout)\n",
    "            densout = LeakyReLU(alpha=0.3)(densout)\n",
    "        densout = Dense(218, activation='linear')(densout)\n",
    "        out_layer = LeakyReLU(alpha=0.3)(densout)\n",
    "        NN = tf.keras.models.Model(inpU, out_layer)\n",
    "        print('NN is ',NN.summary())\n",
    "        \n",
    "        # 2) Define loss\n",
    "        al = alpha/4 # Weight given to each residual\n",
    "        Loss = WeakLoss(inpU, inp_div=train_gen.input_transform.div,\n",
    "                            inp_sub=train_gen.input_transform.sub,\n",
    "                            norm_q=scale_dict['PHQ'],\n",
    "                            hyai=hyai, hybi=hybi, name='loss',\n",
    "                            alpha_mass=al, alpha_ent=al,\n",
    "                            alpha_lw=al, alpha_sw=al)\n",
    "        \n",
    "        # 3) Compile model\n",
    "        NN.compile(tf.keras.optimizers.RMSprop(), loss=Loss, metrics=[mse])\n",
    "        \n",
    "        # 4) Train model\n",
    "        NN.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        # 5) Save model\n",
    "        #path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'.h5'\n",
    "        #NN.save(path)\n",
    "        #print('NN saved in ',path)\n",
    "        print('Finished')\n",
    "\n",
    "pr.disable()\n",
    "pr.print_stats(sort='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
