{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/20/2020 - Adapting Ankitesh's climate-invariant training notebook for hyperparameter optimization by David Walling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/oasis/scratch/comet/tbeucler/temp_project/CBRAIN-CAM/notebooks/tbeucler_devlog\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,\"/home1/07064/tg863631/anaconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages\") #work around for h5py\n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "                    decode_times=False)\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "scale_dict = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dim_size = 40 #required for interpolation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorClimInv(DataGenerator):\n",
    "    \n",
    "    def __init__(self, data_fn, input_vars, output_vars,\n",
    "             norm_fn=None, input_transform=None, output_transform=None,\n",
    "             batch_size=1024, shuffle=True, xarray=False, var_cut_off=None, \n",
    "             rh_trans=True,t2tns_trans=True,\n",
    "             lhflx_trans=True,\n",
    "             scaling=True,interpolate=True,\n",
    "             hyam=None,hybm=None,                 \n",
    "             inp_subRH=None,inp_divRH=None,\n",
    "             inp_subTNS=None,inp_divTNS=None,\n",
    "             lev=None, interm_size=40,\n",
    "             lower_lim=6,\n",
    "             is_continous=True,Tnot=5,\n",
    "                mode='train'):\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        self.interpolate = interpolate\n",
    "        self.rh_trans = rh_trans\n",
    "        self.t2tns_trans = t2tns_trans\n",
    "        self.lhflx_trans = lhflx_trans\n",
    "        self.inp_shape = 64\n",
    "        self.mode=mode\n",
    "        super().__init__(data_fn, input_vars,output_vars,norm_fn,input_transform,output_transform,\n",
    "                        batch_size,shuffle,xarray,var_cut_off) ## call the base data generator\n",
    "        self.inp_sub = self.input_transform.sub\n",
    "        self.inp_div = self.input_transform.div\n",
    "        if rh_trans:\n",
    "            self.qv2rhLayer = QV2RHNumpy(self.inp_sub,self.inp_div,inp_subRH,inp_divRH,hyam,hybm)\n",
    "        \n",
    "        if lhflx_trans:\n",
    "            self.lhflxLayer = LhflxTransNumpy(self.inp_sub,self.inp_div,hyam,hybm)\n",
    "            \n",
    "        if t2tns_trans:\n",
    "            self.t2tnsLayer = T2TmTNSNumpy(self.inp_sub,self.inp_div,inp_subTNS,inp_divTNS,hyam,hybm)\n",
    "            \n",
    "        if scaling:\n",
    "            self.scalingLayer = ScalingNumpy(hyam,hybm)\n",
    "            self.inp_shape += 1\n",
    "                    \n",
    "        if interpolate:\n",
    "            self.interpLayer = InterpolationNumpy(lev,is_continous,Tnot,lower_lim,interm_size)\n",
    "            self.inp_shape += interm_size*2 + 4 + 30 ## 4 same as 60-64 and 30 for lev_tilde.size\n",
    "        \n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "        # Normalize\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result) \n",
    "            \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            \n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        if self.mode=='val':\n",
    "            return xr.DataArray(X_result), xr.DataArray(Y)\n",
    "        return X_result,Y\n",
    "    \n",
    "    ##transforms the input data into the required format, take the unnormalized dataset\n",
    "    def transform(self,X):\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result)  \n",
    "        \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "\n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        return X_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose between aquaplanet and realistic geography here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_aquaplanet = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'\n",
    "path_realgeography = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/geography'\n",
    "\n",
    "path = path_aquaplanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = path+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For positve sepearation (required since we are going to use scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE_RH = 'PosCRH_CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'PosCRH_CI_RH_M4K_NORM_norm.nc'\n",
    "\n",
    "train_gen_RH_pos = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = TRAINDIR+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For negative sepearation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE_RH = 'NegCRH_CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'NegCRH_CI_RH_M4K_NORM_norm.nc'\n",
    "\n",
    "train_gen_RH_neg = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = TRAINDIR+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_TNS = 'CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'CI_TNS_M4K_NORM_norm.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE_TNS = 'PosCRH_CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'PosCRH_CI_TNS_M4K_NORM_norm.nc'\n",
    "\n",
    "train_gen_TNS_pos = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE_TNS = 'NegCRH_CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'NegCRH_CI_TNS_M4K_NORM_norm.nc'\n",
    "\n",
    "train_gen_TNS_neg = DataGenerator(\n",
    "    data_fn = TRAINDIR+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator Combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this won't be used just to show we can use it overall\n",
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_M4K_valid.nc'\n",
    "\n",
    "train_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div\n",
    ")\n",
    "\n",
    "valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 179)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'PosCRH_CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'PosCRH_CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'PosCRH_CI_SP_M4K_valid.nc'\n",
    "\n",
    "train_gen_pos = DataGeneratorClimInv(\n",
    "    data_fn = TRAINDIR+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH_pos.input_transform.sub, inp_divRH=train_gen_RH_pos.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS_pos.input_transform.sub,inp_divTNS=train_gen_TNS_pos.input_transform.div,\n",
    "    is_continous=True\n",
    ")\n",
    "\n",
    "valid_gen_pos = DataGeneratorClimInv(\n",
    "    data_fn = TRAINDIR+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH_pos.input_transform.sub, inp_divRH=train_gen_RH_pos.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS_pos.input_transform.sub,inp_divTNS=train_gen_TNS_pos.input_transform.div,\n",
    "    is_continous=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 179)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen_pos[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'NegCRH_CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'NegCRH_CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'NegCRH_CI_SP_M4K_valid.nc'\n",
    "\n",
    "### we don't scale this network\n",
    "train_gen_neg = DataGeneratorClimInv(\n",
    "    data_fn = TRAINDIR+TRAINFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH_neg.input_transform.sub, inp_divRH=train_gen_RH_neg.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS_neg.input_transform.sub,inp_divTNS=train_gen_TNS_neg.input_transform.div,\n",
    "    is_continous=True,\n",
    "    scaling=False\n",
    ")\n",
    "\n",
    "valid_gen_neg = DataGeneratorClimInv(\n",
    "    data_fn = TRAINDIR+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH_neg.input_transform.sub, inp_divRH=train_gen_RH_neg.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS_neg.input_transform.sub,inp_divTNS=train_gen_TNS_neg.input_transform.div,\n",
    "    is_continous=True,\n",
    "    scaling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 178)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen_neg[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(179,)) ## input after rh and tns transformation\n",
    "offset = 65\n",
    "inp_TNS = inp[:,offset:offset+2*inter_dim_size+4]\n",
    "offset = offset+2*inter_dim_size+4\n",
    "lev_tilde_before = inp[:,offset:offset+30]\n",
    "offset = offset+30\n",
    "\n",
    "densout = Dense(128, activation='linear')(inp_TNS)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "denseout = Dense(2*inter_dim_size+4, activation='linear')(densout)\n",
    "lev_original_out = reverseInterpLayer(inter_dim_size)([denseout,lev_tilde_before])\n",
    "out = ScaleOp(OpType.PWA.value,\n",
    "              inp_subQ=train_gen_pos.input_transform.sub, \n",
    "              inp_divQ=train_gen_pos.input_transform.div,\n",
    "              )([inp,lev_original_out])\n",
    "model_pos = tf.keras.models.Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 179)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 84)]         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          10880       tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          16512       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16512       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          16512       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 128)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 84)           10836       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 30)]         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reverse_interp_layer (reverseIn (None, 64)           0           dense_7[0][0]                    \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "scale_op (ScaleOp)              (None, 64)           0           input_1[0][0]                    \n",
      "                                                                 reverse_interp_layer[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 120,788\n",
      "Trainable params: 120,788\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_pos.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pos.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'CI_Pos_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:From /home/tbeucler/pkg/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "9144/9144 [==============================] - 1766s 193ms/step - loss: 623.9487 - val_loss: 538.1217\n",
      "Epoch 2/25\n",
      " 610/9144 [=>............................] - ETA: 16:30 - loss: 558.1734"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6d02dd811136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model_pos.fit_generator(train_gen_pos, epochs=Nep, validation_data=valid_gen_pos,\\\n\u001b[0;32m----> 3\u001b[0;31m               callbacks=[earlyStopping, mcp_save])\n\u001b[0m",
      "\u001b[0;32m~/pkg/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/pkg/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0;31m# Callbacks batch begin.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    240\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/pkg/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \"\"\"\n\u001b[1;32m   3494\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3495\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3401\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3403\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3404\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/miniconda3/envs/CbrainCustomLayer/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3516\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m         \u001b[0mkth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moverwrite_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Nep = 25\n",
    "model_pos.fit_generator(train_gen_pos, epochs=Nep, validation_data=valid_gen_pos,\\\n",
    "              callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(178,)) ## input after rh and tns transformation\n",
    "offset = 64\n",
    "inp_TNS = inp[:,offset:offset+2*inter_dim_size+4]\n",
    "offset = offset+2*inter_dim_size+4\n",
    "lev_tilde_before = inp[:,offset:offset+30]\n",
    "offset = offset+30\n",
    "\n",
    "densout = Dense(128, activation='linear')(inp_TNS)\n",
    "densout = LeakyReLU(alpha=0.3)(densout)\n",
    "for i in range (6):\n",
    "    densout = Dense(128, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "denseout = Dense(2*inter_dim_size+4, activation='linear')(densout)\n",
    "lev_original_out = reverseInterpLayer(inter_dim_size)([denseout,lev_tilde_before])\n",
    "\n",
    "model_neg = tf.keras.models.Model(inp, lev_original_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 178)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 84)]         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          10880       tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          16512       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16512       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          16512       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 128)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 84)           10836       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 30)]         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reverse_interp_layer (reverseIn (None, 64)           0           dense_7[0][0]                    \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "==================================================================================================\n",
      "Total params: 120,788\n",
      "Trainable params: 120,788\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_neg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg.compile(tf.keras.optimizers.Adam(), loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'CI_Neg_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_HDF5 = '/oasis/scratch/comet/ankitesh/temp_project/models/Comnined/'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 3268/32231 [==>...........................] - ETA: 1:32:07 - loss: 315.9958"
     ]
    }
   ],
   "source": [
    "Nep = 10\n",
    "model_neg.fit_generator(train_gen_neg, epochs=Nep, validation_data=valid_gen_neg,\\\n",
    "              callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to combine positive and negative NNs to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define how to load climate-invariant NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateNet:\n",
    "    def __init__(self,dict_lay,data_fn,config_fn,\n",
    "             lev,hyam,hybm,TRAINDIR,\n",
    "             nlat, nlon, nlev, ntime,\n",
    "             inp_subRH,inp_divRH,\n",
    "             inp_subTNS,inp_divTNS,\n",
    "             rh_trans=False,t2tns_trans=False,\n",
    "             lhflx_trans=False,\n",
    "             scaling=False,interpolate=False,\n",
    "             model=None,\n",
    "             pos_model=None,neg_model=None,\n",
    "                ):\n",
    "        \n",
    "        \n",
    "        with open(config_fn, 'r') as f:\n",
    "            config = yaml.load(f)\n",
    "        out_scale_dict = load_pickle(config['output_dict'])\n",
    "        ngeo = nlat * nlon\n",
    "        in_vars = config['inputs']\n",
    "        out_vars = config['outputs']\n",
    "\n",
    "        self.valid_gen = DataGeneratorClimInv(\n",
    "                data_fn = data_fn,\n",
    "                input_vars=in_vars,\n",
    "                output_vars=out_vars,\n",
    "                norm_fn=config['data_dir'] + config['norm_fn'],\n",
    "                input_transform=(config['input_sub'], config['input_div']),\n",
    "                output_transform=out_scale_dict,\n",
    "                batch_size=ngeo,\n",
    "                shuffle=False,\n",
    "                xarray=True,\n",
    "                var_cut_off=config['var_cut_off'] if 'var_cut_off' in config.keys() else None,\n",
    "                rh_trans = rh_trans,t2tns_trans = t2tns_trans,\n",
    "                lhflx_trans = lhflx_trans,\n",
    "                scaling = scaling,\n",
    "                lev=lev,interpolate = interpolate,\n",
    "                hyam=hyam,hybm=hybm,\n",
    "                inp_subRH=inp_subRH, inp_divRH=inp_divRH,\n",
    "                inp_subTNS=inp_subTNS,inp_divTNS=inp_divTNS,\n",
    "                mode='val'                \n",
    "                \n",
    "        )\n",
    "\n",
    "        self.rh_trans = rh_trans\n",
    "        self.t2tns_trans = t2tns_trans\n",
    "        self.lhflx_trans = lhflx_trans\n",
    "        self.scaling = scaling\n",
    "        self.interpolate = interpolate\n",
    "        self.subQ,self.divQ = np.array(self.valid_gen.input_transform.sub),np.array(self.valid_gen.input_transform.div)\n",
    "\n",
    "        if model != None:\n",
    "            self.model = load_model(model,custom_objects=dict_lay)\n",
    "            \n",
    "        if scaling:\n",
    "            self.pos_model = load_model(pos_model,custom_objects=dict_lay)\n",
    "            self.neg_model = load_model(neg_model,custom_objects=dict_lay)\n",
    "         \n",
    "            #just for the norm values\n",
    "            self.pos_data_gen = DataGeneratorClimInv(\n",
    "                                data_fn = TRAINDIR+'PosCRH_CI_SP_M4K_train_shuffle.nc',\n",
    "                                input_vars = in_vars,\n",
    "                                output_vars = out_vars,\n",
    "                                norm_fn = TRAINDIR+'PosCRH_CI_SP_M4K_NORM_norm.nc',\n",
    "                                input_transform = ('mean', 'maxrs'),\n",
    "                                output_transform = out_scale_dict,\n",
    "                                batch_size=1024,\n",
    "                                shuffle=True,\n",
    "                                lev=lev,\n",
    "                                hyam=hyam,hybm=hybm,\n",
    "                                inp_subRH=train_gen_RH_pos.input_transform.sub, inp_divRH=train_gen_RH_pos.input_transform.div,\n",
    "                                inp_subTNS=train_gen_TNS_pos.input_transform.sub,inp_divTNS=train_gen_TNS_pos.input_transform.div,\n",
    "                                is_continous=True,\n",
    "                                scaling=True,\n",
    "                                interpolate=interpolate,\n",
    "                                rh_trans=rh_trans,\n",
    "                                t2tns_trans=t2tns_trans,\n",
    "                                lhflx_trans=lhflx_trans\n",
    "                            )\n",
    "        \n",
    "            self.neg_data_gen = DataGeneratorClimInv(\n",
    "                                data_fn = TRAINDIR+'NegCRH_CI_SP_M4K_train_shuffle.nc',\n",
    "                                input_vars = in_vars,\n",
    "                                output_vars = out_vars,\n",
    "                                norm_fn = TRAINDIR+'NegCRH_CI_SP_M4K_NORM_norm.nc',\n",
    "                                input_transform = ('mean', 'maxrs'),\n",
    "                                output_transform = out_scale_dict,\n",
    "                                batch_size=1024,\n",
    "                                shuffle=True,\n",
    "                                lev=lev,\n",
    "                                hyam=hyam,hybm=hybm,\n",
    "                                inp_subRH=train_gen_RH_neg.input_transform.sub, inp_divRH=train_gen_RH_neg.input_transform.div,\n",
    "                                inp_subTNS=train_gen_TNS_neg.input_transform.sub,inp_divTNS=train_gen_TNS_neg.input_transform.div,\n",
    "                                is_continous=True,\n",
    "                                interpolate=interpolate,\n",
    "                                scaling=False,\n",
    "                                rh_trans=rh_trans,\n",
    "                                t2tns_trans=t2tns_trans,\n",
    "                                lhflx_trans=lhflx_trans\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    def reorder(self,op_pos,op_neg,mask):\n",
    "        op = []\n",
    "        pos_i=0\n",
    "        neg_i = 0\n",
    "        for m in mask:\n",
    "            if m:\n",
    "                op.append(op_pos[pos_i])\n",
    "                pos_i += 1\n",
    "            else:\n",
    "                op.append(op_neg[neg_i])\n",
    "                neg_i += 1\n",
    "        return np.array(op)\n",
    "                \n",
    "                \n",
    "    def predict_on_batch(self,inp):\n",
    "        inp_de = inp*self.divQ+self.subQ\n",
    "        if not self.scaling:\n",
    "            inp_pred = self.valid_gen.transform(inp_de)           \n",
    "            return self.model.predict_on_batch(inp_pred)\n",
    "        \n",
    "\n",
    "        mask = ScalingNumpy(hyam,hybm).crh(inp_de)> 0.8\n",
    "        pos_inp = inp[mask]\n",
    "        neg_inp = inp[np.logical_not(mask)]\n",
    "        ### for positive\n",
    "        pos_inp = pos_inp*self.divQ + self.subQ\n",
    "        pos_inp = self.pos_data_gen.transform(pos_inp)\n",
    "        op_pos = self.pos_model.predict_on_batch(pos_inp)\n",
    "        neg_inp = neg_inp*self.divQ + self.subQ\n",
    "        neg_inp = self.neg_data_gen.transform(neg_inp)\n",
    "        op_neg = self.neg_model.predict_on_batch(neg_inp)\n",
    "        op = self.reorder(np.array(op_pos),np.array(op_neg),mask)\n",
    "        return op\n",
    "\n",
    "    \n",
    "    \n",
    "    ##just for network is scaling is present\n",
    "    def predict_on_batch_seperate(self,inp):\n",
    "        if self.scaling==False:\n",
    "            raise(\"Scaling is not present in this model\")\n",
    "        \n",
    "        inp_de = inp*self.divQ + self.subQ\n",
    "        mask = ScalingNumpy(hyam,hybm).crh(inp_de)> 0.8\n",
    "        pos_inp = inp[mask]\n",
    "        neg_inp = inp[np.logical_not(mask)]\n",
    "        \n",
    "        pos_inp = pos_inp*self.divQ + self.subQ\n",
    "        pos_inp = self.pos_data_gen.transform(pos_inp)\n",
    "        neg_inp = neg_inp*self.divQ + self.subQ\n",
    "        neg_inp = self.neg_data_gen.transform(neg_inp)\n",
    "        \n",
    "        op_pos = self.pos_model.predict_on_batch(pos_inp)\n",
    "        op_neg = self.neg_model.predict_on_batch(neg_inp)\n",
    "        \n",
    "        return mask,op_pos,op_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_climate_model(dict_lay,config_fn,data_fn,lev,hyam,hybm,TRAINDIR,\n",
    "                       inp_subRH,inp_divRH,\n",
    "                       inp_subTNS,inp_divTNS,\n",
    "                       nlat=64, nlon=128, nlev=30, ntime=48,\n",
    "                        rh_trans=False,t2tns_trans=False,\n",
    "                        lhflx_trans=False,\n",
    "                        scaling=False,interpolate=False,\n",
    "                        model=None,\n",
    "                        pos_model=None,neg_model=None):\n",
    "    \n",
    "    \n",
    "    \n",
    "    obj = ClimateNet(dict_lay,data_fn,config_fn,\n",
    "                     lev,hyam,hybm,TRAINDIR,\n",
    "                     nlat, nlon, nlev, ntime,\n",
    "                     inp_subRH,inp_divRH,\n",
    "                     inp_subTNS,inp_divTNS,\n",
    "                    rh_trans=rh_trans,t2tns_trans=t2tns_trans,\n",
    "                    lhflx_trans=lhflx_trans, scaling=scaling,\n",
    "                    interpolate=interpolate,\n",
    "                    model = model,\n",
    "                    pos_model=pos_model,neg_model=neg_model)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models' paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'CI_SP_M4K_CONFIG.yml' # Configuration file\n",
    "data_file = ['CI_SP_M4K_valid.nc','CI_SP_P4K_valid.nc']  # Validation/test data sets\n",
    "NNarray = ['CI_Pos_temp.hdf5*CI_Neg_temp.hdf5'] # NN to evaluate \n",
    "NNname = ['Combined NN'] # Name of NNs for plotting\n",
    "dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer,\n",
    "            'QV2RH':QV2RH,'T2TmTNS':T2TmTNS,'eliq':eliq,'eice':eice,'esat':esat,'qv':qv,'RH':RH,\n",
    "           'reverseInterpLayer':reverseInterpLayer,'ScaleOp':ScaleOp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of different variables\n",
    "PHQ_idx = slice(0,30)\n",
    "TPHYSTND_idx = slice(30,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
