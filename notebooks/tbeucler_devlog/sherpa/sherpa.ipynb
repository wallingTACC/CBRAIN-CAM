{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/15/2020\n",
    "- Adapting Ankitesh's notebook that builds and train a \"brute-force\" network to David Walling's hyperparameter search  \n",
    "- Adding the option to choose between aquaplanet and real-geography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/00157/walling/projects/cloud_emulator/walling-CBRAIN-CAM\n"
     ]
    }
   ],
   "source": [
    "DEV_PATH = '~walling/cloud/walling-CBRAIN-CAM'\n",
    "CONDA_PATH = '/work/00157/walling/projects/cloud_emulator/CbrainCustomLayer'\n",
    "%cd '~walling/cloud/walling-CBRAIN-CAM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/00157/walling/conda/envs/cbrain_env09192019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/work/00157/walling/conda/envs/cbrain_env09192019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/work/00157/walling/conda/envs/cbrain_env09192019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/work/00157/walling/conda/envs/cbrain_env09192019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/work/00157/walling/conda/envs/cbrain_env09192019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/work/00157/walling/conda/envs/cbrain_env09192019/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/00157/walling/projects/cloud_emulator/walling-CBRAIN-CAM\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-86783a9571ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxarray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_probability'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,CONDA_PATH + \"/lib/python3.7/site-packages\") #work around for h5py\n",
    "sys.path.insert(1, DEV_PATH)\n",
    "sys.path.insert(1, DEV_PATH + \"/cbrain\")\n",
    "sys.path.insert(1, DEV_PATH + \"/notebooks/tbeucler_devlog\")\n",
    "                \n",
    "from cbrain.imports import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.utils import *\n",
    "from cbrain.layers import *\n",
    "from cbrain.data_generator import DataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_probability as tfp\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imag\n",
    "import scipy.integrate as sin\n",
    "# import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import pickle\n",
    "# from climate_invariant import *\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from climate_invariant_utils import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-41494c8c96ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_probability'"
     ]
    }
   ],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates (just pick any file from the climate model run)\n",
    "coor = xr.open_dataset(\"/oasis/scratch/comet/ankitesh/temp_project/data/sp8fbp_minus4k.cam2.h1.0000-01-01-00000.nc\",\\\n",
    "                    decode_times=False)\n",
    "lat = coor.lat; lon = coor.lon; lev = coor.lev;\n",
    "coor.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDIR = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/CRHData/'\n",
    "path = '/home/ankitesh/CBrain_project/CBRAIN-CAM/cbrain/'\n",
    "\n",
    "# Load hyam and hybm to calculate pressure field in SPCAM\n",
    "path_hyam = 'hyam_hybm.pkl'\n",
    "hf = open(path+path_hyam,'rb')\n",
    "hyam,hybm = pickle.load(hf)\n",
    "\n",
    "# Scale dictionary to convert the loss to W/m2\n",
    "scale_dict = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Data generator class for the climate-invariant network. Calculates the physical rescalings needed to make the NN climate-invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorClimInv(DataGenerator):\n",
    "    \n",
    "    def __init__(self, data_fn, input_vars, output_vars,\n",
    "             norm_fn=None, input_transform=None, output_transform=None,\n",
    "             batch_size=1024, shuffle=True, xarray=False, var_cut_off=None,\n",
    "             rh_trans=True,t2tns_trans=True,\n",
    "             lhflx_trans=True,\n",
    "             scaling=True,interpolate=True,\n",
    "             hyam=None,hybm=None,                 \n",
    "             inp_subRH=None,inp_divRH=None,\n",
    "             inp_subTNS=None,inp_divTNS=None,\n",
    "             lev=None, interm_size=40,\n",
    "             lower_lim=6,\n",
    "             is_continous=True,Tnot=5,\n",
    "                mode='train'):\n",
    "        \n",
    "        self.scaling = scaling\n",
    "        self.interpolate = interpolate\n",
    "        self.rh_trans = rh_trans\n",
    "        self.t2tns_trans = t2tns_trans\n",
    "        self.lhflx_trans = lhflx_trans\n",
    "        self.inp_shape = 64\n",
    "        self.mode=mode\n",
    "        super().__init__(data_fn, input_vars,output_vars,norm_fn,input_transform,output_transform,\n",
    "                        batch_size,shuffle,xarray,var_cut_off) ## call the base data generator\n",
    "        self.inp_sub = self.input_transform.sub\n",
    "        self.inp_div = self.input_transform.div\n",
    "        if rh_trans:\n",
    "            self.qv2rhLayer = QV2RHNumpy(self.inp_sub,self.inp_div,inp_subRH,inp_divRH,hyam,hybm)\n",
    "        \n",
    "        if lhflx_trans:\n",
    "            self.lhflxLayer = LhflxTransNumpy(self.inp_sub,self.inp_div,hyam,hybm)\n",
    "            \n",
    "        if t2tns_trans:\n",
    "            self.t2tnsLayer = T2TmTNSNumpy(self.inp_sub,self.inp_div,inp_subTNS,inp_divTNS,hyam,hybm)\n",
    "            \n",
    "        if scaling:\n",
    "            self.scalingLayer = ScalingNumpy(hyam,hybm)\n",
    "            self.inp_shape += 1\n",
    "                    \n",
    "        if interpolate:\n",
    "            self.interpLayer = InterpolationNumpy(lev,is_continous,Tnot,lower_lim,interm_size)\n",
    "            self.inp_shape += interm_size*2 + 4 + 30 ## 4 same as 60-64 and 30 for lev_tilde.size\n",
    "        \n",
    "            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Compute start and end indices for batch\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "\n",
    "        # Grab batch from data\n",
    "        batch = self.data_ds['vars'][start_idx:end_idx]\n",
    "\n",
    "        # Split into inputs and outputs\n",
    "        X = batch[:, self.input_idxs]\n",
    "        Y = batch[:, self.output_idxs]\n",
    "        # Normalize\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        Y = self.output_transform.transform(Y)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result) \n",
    "            \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            X_result = X_result[:,:64]\n",
    "            X = X[:,:64]\n",
    "            \n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        if self.mode=='val':\n",
    "            return xr.DataArray(X_result), xr.DataArray(Y)\n",
    "        return X_result,Y\n",
    "    \n",
    "    ##transforms the input data into the required format, take the unnormalized dataset\n",
    "    def transform(self,X):\n",
    "        X_norm = self.input_transform.transform(X)\n",
    "        X_result = X_norm\n",
    "        \n",
    "        if self.rh_trans:\n",
    "            X_result = self.qv2rhLayer.process(X_result)  \n",
    "        \n",
    "        if self.lhflx_trans:\n",
    "            X_result = self.lhflxLayer.process(X_result)\n",
    "            X_result = X_result[:,:64]\n",
    "            X = X[:,:64]\n",
    "\n",
    "        if self.t2tns_trans:\n",
    "            X_result = self.t2tnsLayer.process(X_result)\n",
    "        \n",
    "        if self.scaling:\n",
    "            scalings = self.scalingLayer.process(X) \n",
    "            X_result = np.hstack((X_result,scalings))\n",
    "        \n",
    "        if self.interpolate:\n",
    "            interpolated = self.interpLayer.process(X,X_result)\n",
    "            X_result = np.hstack((X_result,interpolated))\n",
    "            \n",
    "\n",
    "        return X_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose between aquaplanet and realistic geography here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_aquaplanet = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/'\n",
    "path_realgeography = '/oasis/scratch/comet/ankitesh/temp_project/PrepData/geography'\n",
    "\n",
    "path = path_aquaplanet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict_RH = load_pickle('/home/ankitesh/CBrain_project/CBRAIN-CAM/nn_config/scale_dicts/009_Wm2_scaling.pkl')\n",
    "scale_dict_RH['RH'] = 0.01*L_S/G, # Arbitrary 0.1 factor as specific humidity is generally below 2%\n",
    "\n",
    "in_vars_RH = ['RH','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars_RH = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_RH = 'CI_RH_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_RH = 'CI_RH_M4K_NORM_norm.nc'\n",
    "VALIDFILE_RH = 'CI_RH_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_RH = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_RH,\n",
    "    input_vars = in_vars_RH,\n",
    "    output_vars = out_vars_RH,\n",
    "    norm_fn = path+NORMFILE_RH,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict_RH,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator using TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TfromNS','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']\n",
    "\n",
    "TRAINFILE_TNS = 'CI_TNS_M4K_NORM_train_shuffle.nc'\n",
    "NORMFILE_TNS = 'CI_TNS_M4K_NORM_norm.nc'\n",
    "VALIDFILE_TNS = 'CI_TNS_M4K_NORM_valid.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_TNS = DataGenerator(\n",
    "    data_fn = path+TRAINFILE_TNS,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE_TNS,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = 'CI_SP_M4K_train_shuffle.nc'\n",
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_M4K_valid.nc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sherpa.core:\n",
      "-------------------------------------------------------\n",
      "SHERPA Dashboard running. Access via\n",
      "http://198.202.119.245:8880 if on a cluster or\n",
      "http://localhost:8880 if running locally.\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"sherpa.app.app\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:werkzeug:198.108.66.193 - - [11/May/2020 11:21:25] code 400, message Bad HTTP/0.9 request type ('\\x16\\x03\\x01\\x00{\\x01\\x00\\x00w\\x03\\x03«l¼2X')\n"
     ]
    }
   ],
   "source": [
    "import sherpa\n",
    "parameters = [sherpa.Discrete('num_units', [64, 256]),\n",
    "              sherpa.Discrete('num_layers', [6, 24]),\n",
    "              sherpa.Ordinal('batch_size', [1024, 2048, 4096, 8192]),]\n",
    "#alg = sherpa.algorithms.BayesianOptimization(max_num_trials=50)\n",
    "alg = sherpa.algorithms.GPyOpt(max_num_trials=50)\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=alg,\n",
    "                     lower_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size = 2048\n",
      "Num Layers = 19\n",
      "Num Units = 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20688/20688 [==============================] - 2332s 113ms/step - loss: 437.1684 - val_loss: 375.6745\n",
      "Epoch 2/10\n",
      "20688/20688 [==============================] - 2341s 113ms/step - loss: 375.7685 - val_loss: 364.1915\n",
      "Epoch 3/10\n",
      "20688/20688 [==============================] - 3203s 155ms/step - loss: 366.2817 - val_loss: 364.7692\n",
      "Epoch 4/10\n",
      "20688/20688 [==============================] - 2976s 144ms/step - loss: 360.6420 - val_loss: 363.9541\n",
      "Epoch 5/10\n",
      "20688/20688 [==============================] - 3864s 187ms/step - loss: 356.8447 - val_loss: 356.7396\n",
      "Epoch 6/10\n",
      "20688/20688 [==============================] - 3880s 188ms/step - loss: 354.0231 - val_loss: 352.7654\n",
      "Epoch 7/10\n",
      "20688/20688 [==============================] - 2602s 126ms/step - loss: 351.6275 - val_loss: 347.9559\n",
      "Epoch 8/10\n",
      "20688/20688 [==============================] - 2180s 105ms/step - loss: 349.5590 - val_loss: 346.3438\n",
      "Epoch 9/10\n",
      "20688/20688 [==============================] - 2199s 106ms/step - loss: 36416115.0195 - val_loss: 22885.3253\n",
      "Epoch 10/10\n",
      "20688/20688 [==============================] - 2577s 125ms/step - loss: 3574.7240 - val_loss: 628.4750\n",
      "Batch Size = 1024\n",
      "Num Layers = 18\n",
      "Num Units = 168\n",
      "Epoch 1/10\n",
      "41376/41376 [==============================] - 4339s 105ms/step - loss: 421.4751 - val_loss: 390.1181\n",
      "Epoch 2/10\n",
      "41376/41376 [==============================] - 5259s 127ms/step - loss: 376.9934 - val_loss: 375.3774\n",
      "Epoch 3/10\n",
      "41376/41376 [==============================] - 5881s 142ms/step - loss: 368.4301 - val_loss: 360.8473\n",
      "Epoch 4/10\n",
      "41376/41376 [==============================] - 5506s 133ms/step - loss: 362.9498 - val_loss: 357.3685\n",
      "Epoch 5/10\n",
      "41376/41376 [==============================] - 4123s 100ms/step - loss: 358.7865 - val_loss: 354.8742\n",
      "Epoch 6/10\n",
      "41376/41376 [==============================] - 4421s 107ms/step - loss: 355.8758 - val_loss: 356.6642\n",
      "Epoch 7/10\n",
      "41376/41376 [==============================] - 5458s 132ms/step - loss: 353.6529 - val_loss: 348.3231\n",
      "Epoch 8/10\n",
      "41376/41376 [==============================] - 4533s 110ms/step - loss: 351.8553 - val_loss: 351.8797\n",
      "Epoch 9/10\n",
      "41376/41376 [==============================] - 3937s 95ms/step - loss: 350.3924 - val_loss: 347.2131\n",
      "Epoch 10/10\n",
      "41376/41376 [==============================] - 3355s 81ms/step - loss: 349.3083 - val_loss: 347.6017\n",
      "Batch Size = 8192\n",
      "Num Layers = 18\n",
      "Num Units = 69\n",
      "Epoch 1/10\n",
      "5172/5172 [==============================] - 646s 125ms/step - loss: 588.6622 - val_loss: 452.2048\n",
      "Epoch 2/10\n",
      "5172/5172 [==============================] - 706s 136ms/step - loss: 441.6477 - val_loss: 422.0238\n",
      "Epoch 3/10\n",
      "5172/5172 [==============================] - 919s 178ms/step - loss: 419.9105 - val_loss: 407.9861\n",
      "Epoch 4/10\n",
      "5172/5172 [==============================] - 1006s 195ms/step - loss: 404.9244 - val_loss: 395.6370\n",
      "Epoch 5/10\n",
      "5172/5172 [==============================] - 396s 76ms/step - loss: 396.2426 - val_loss: 389.7102\n",
      "Epoch 6/10\n",
      "5172/5172 [==============================] - 396s 77ms/step - loss: 391.1886 - val_loss: 387.5658\n",
      "Epoch 7/10\n",
      "5172/5172 [==============================] - 595s 115ms/step - loss: 386.3591 - val_loss: 384.9833\n",
      "Epoch 8/10\n",
      "5172/5172 [==============================] - 711s 137ms/step - loss: 382.1928 - val_loss: 375.2221\n",
      "Epoch 9/10\n",
      "5172/5172 [==============================] - 627s 121ms/step - loss: 378.1634 - val_loss: 373.5921\n",
      "Epoch 10/10\n",
      "5172/5172 [==============================] - 708s 137ms/step - loss: 374.9773 - val_loss: 376.5427\n",
      "Batch Size = 2048\n",
      "Num Layers = 21\n",
      "Num Units = 70\n",
      "Epoch 1/10\n",
      "20688/20688 [==============================] - 1907s 92ms/step - loss: 477.4694 - val_loss: 407.6641\n",
      "Epoch 2/10\n",
      " 6419/20688 [========>.....................] - ETA: 20:06 - loss: 410.1026"
     ]
    }
   ],
   "source": [
    "for trial in study:\n",
    "    \n",
    "    print('Batch Size = ' + str(trial.parameters['batch_size']))\n",
    "    print('Num Layers = ' + str(trial.parameters['num_layers']))\n",
    "    print('Num Units = ' + str(trial.parameters['num_units']))\n",
    "    \n",
    "    train_gen = DataGeneratorClimInv(\n",
    "        data_fn = path+TRAINFILE,\n",
    "        input_vars = in_vars,\n",
    "        output_vars = out_vars,\n",
    "        norm_fn = path+NORMFILE,\n",
    "        input_transform = ('mean', 'maxrs'),\n",
    "        output_transform = scale_dict,\n",
    "        batch_size=trial.parameters['batch_size'],\n",
    "        shuffle=True,\n",
    "        lev=lev,\n",
    "        hyam=hyam,hybm=hybm,\n",
    "        inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "        inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "        rh_trans=False,t2tns_trans=False,\n",
    "        lhflx_trans=False,\n",
    "        scaling=False,\n",
    "        interpolate=False\n",
    "    )\n",
    "\n",
    "    valid_gen = DataGeneratorClimInv(\n",
    "        data_fn = path+VALIDFILE,\n",
    "        input_vars = in_vars,\n",
    "        output_vars = out_vars,\n",
    "        norm_fn = path+NORMFILE,\n",
    "        input_transform = ('mean', 'maxrs'),\n",
    "        output_transform = scale_dict,\n",
    "        batch_size=trial.parameters['batch_size'],\n",
    "        shuffle=True,\n",
    "        lev=lev,\n",
    "        hyam=hyam,hybm=hybm,\n",
    "        inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "        inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "        rh_trans=False,t2tns_trans=False,\n",
    "        lhflx_trans=False,\n",
    "        scaling=False,\n",
    "        interpolate=False\n",
    "    )\n",
    "\n",
    "    inp = Input(shape=(64,)) ## input after rh and tns transformation\n",
    "    #densout = Dense(128, activation='linear')(inp)\n",
    "    densout = Dense(trial.parameters['num_units'], activation='linear')(inp)\n",
    "\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    for i in range(trial.parameters['num_layers']): #range (6):\n",
    "        #densout = Dense(128, activation='linear')(densout)\n",
    "        densout = Dense(trial.parameters['num_units'], activation='linear')(densout)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    dense_out = Dense(64, activation='linear')(densout)\n",
    "    model = tf.keras.models.Model(inp, dense_out)\n",
    "    model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "    Nep = 10\n",
    "    #model.summary()\n",
    "    model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[study.keras_callback(trial, objective_name='val_loss')])\n",
    "    study.finalize(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next 3 cells deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the model\n",
    "path_HDF5 = '/oasis/scratch/comet/tbeucler/temp_project/CBRAIN_models/'\n",
    "save_name = 'BF_temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(), loss=mse)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save_pos = ModelCheckpoint(path_HDF5+save_name+'.hdf5',save_best_only=True, monitor='val_loss', mode='min')\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=0, update_freq=1000,embeddings_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nep = 10\n",
    "model.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen,\\\n",
    "              callbacks=[earlyStopping, mcp_save_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chech on validation dataset (m4K simulation) to make sure the NN was properly fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_HDF5+save_name+'.hdf5')\n",
    "model.evaluate_generator(valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on test dataset (p4k simulation) to test ability of NN to generalize to unseen climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vars = ['QBP','TBP','PS', 'SOLIN', 'SHFLX', 'LHFLX']\n",
    "out_vars = ['PHQ','TPHYSTND','FSNT', 'FSNS', 'FLNT', 'FLNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMFILE = 'CI_SP_M4K_NORM_norm.nc'\n",
    "VALIDFILE = 'CI_SP_P4K_valid.nc'\n",
    "\n",
    "valid_gen = DataGeneratorClimInv(\n",
    "    data_fn = path+VALIDFILE,\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = path+NORMFILE,\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    lev=lev,\n",
    "    hyam=hyam,hybm=hybm,\n",
    "    inp_subRH=train_gen_RH.input_transform.sub, inp_divRH=train_gen_RH.input_transform.div,\n",
    "    inp_subTNS=train_gen_TNS.input_transform.sub,inp_divTNS=train_gen_TNS.input_transform.div,\n",
    "    rh_trans=False,t2tns_trans=False,\n",
    "    lhflx_trans=False,\n",
    "    scaling=False,\n",
    "    interpolate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_HDF5+save_name+'.hdf5')\n",
    "model.evaluate_generator(valid_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
